{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "aqWXbY64bxQg"
   },
   "source": [
    "# **Assignment 1: Email Spam Detection**\n",
    "\n",
    "Members:\n",
    "\n",
    "CASSANDRA\n",
    "\n",
    "HUDA\n",
    "\n",
    "DONGDAO\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LmE26fRhcD4R"
   },
   "source": [
    "This report studies about email spam filtering using machine learning models. In this report, the dataset named ‘Data2.csv’ is implemented in the email spam filtering model. Two supervised learning algorithms, the Naïve Bayes algorithm and the Support Vector Machine (SVM) algorithm, will be used to classify the emails into spam or ham."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "M8Y5KNnY9XwX",
    "outputId": "0100b7be-b7a7-454f-8756-0ff61a9c5fed"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_3DPD6fN9uDH",
    "outputId": "2477d21e-0a71-4ef1-c2ff-073ec9f35f51"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5572, 2)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load data\n",
    "data = pd.DataFrame(pd.read_csv(\"Data2.csv\",encoding = \"ISO-8859-1\"))\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CoohxErs-PKB",
    "outputId": "0a691554-77a7-4f6f-f502-db78412e0f2b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5157, 2)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Removing duplicates\n",
    "data.drop_duplicates(inplace = True)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SW1RjWkM-dka",
    "outputId": "e9807da0-8cda-4232-90a9-6c64dfc4a8af"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Category    0\n",
       "Message     0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check number of missing data\n",
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1KX4vghyeWyS"
   },
   "source": [
    "The first step after the data imported is pre-processing the data. First of all, stop words like “the”, “a”, etc will be eliminated due to its meaningless. Besides, the data cleaning process also involves the conversion of all the letters to lowercase, and tokenisation which means separating text into words or smaller chunks with completely different token. Then, lemmatization will be applied to the data into their root form based on dictionary-based approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pAAzAxKA-jed"
   },
   "outputs": [],
   "source": [
    "#Tokenization\n",
    "def process_msg(msg):\n",
    "  \n",
    "  # Removing punctuations\n",
    "  nopunc = [char for char in msg if char not in string.punctuation]\n",
    "  nopunc = ''.join(nopunc)\n",
    "\n",
    "  # Lowering case\n",
    "  nopunc = nopunc.lower()\n",
    "\n",
    "  # Removing stopwords\n",
    "  clean_msg = [word for word in nopunc.split() if word.lower() not in stopwords.words('english')]\n",
    "\n",
    "  return clean_msg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tPHIzgUj_QJs",
    "outputId": "a7981ab2-2b5f-4e5a-d74b-9ba5a98b17d9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       [go, jurong, point, crazy, available, bugis, n...\n",
       "1                          [ok, lar, joking, wif, u, oni]\n",
       "2       [free, entry, 2, wkly, comp, win, fa, cup, fin...\n",
       "3           [u, dun, say, early, hor, u, c, already, say]\n",
       "4       [nah, dont, think, goes, usf, lives, around, t...\n",
       "                              ...                        \n",
       "5567    [2nd, time, tried, 2, contact, u, u, â£750, po...\n",
       "5568                  [ã¼, b, going, esplanade, fr, home]\n",
       "5569                     [pity, mood, soany, suggestions]\n",
       "5570    [guy, bitching, acted, like, id, interested, b...\n",
       "5571                                   [rofl, true, name]\n",
       "Name: Message, Length: 5572, dtype: object"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['Message'].head().apply(process_msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZgN-SXzDejj5"
   },
   "source": [
    "During the pre-processing of the sentences, the sentences were broken into words, which can then proceed with the step of vectorisation. The purpose of vectorisation, which means converting text data into numerical data, is to count the distinct words and the corresponding frequency of each distinct word in the mails. Vectorization is required since the machine does not understand words. Therefore, the words need to be represented by numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DHciaE8kARQr"
   },
   "outputs": [],
   "source": [
    "# Vectorization\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "messages_bow = CountVectorizer(analyzer = process_msg).fit_transform(data['Message'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LQ19K9RvABt2"
   },
   "outputs": [],
   "source": [
    "# Split data into 80% training and 20% testing data\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(messages_bow,data['Category'], test_size = 0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UXUVIr3VhNSj"
   },
   "source": [
    "Naïve Bayes is one of the most widely-used supervised learning methods for classification. Their classifier is naive because it assumes that the connected contingencies are independent of one another. The computation of overall document feasibility would be the substance of merging all of the single word feasibility reports in the file. These Naïve Bayesian classifiers have been widely employed in sentiment categorization since they have less computational power than other algorithms, yet independence assumptions will lead to inaccurate results ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tOoohPzgBKiz",
    "outputId": "86b1525e-4741-4d16-8d56-9df8ec126d82"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB()"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Naive Bayes Classifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "classifierNB = MultinomialNB()\n",
    "classifierNB.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LXuYY6KECUuI",
    "outputId": "419ec9ce-7ac8-4273-9e64-384cef730301"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes Model Assessment\n",
      "Train data set:\n",
      "\n",
      "Predictions:\t ['ham' 'ham' 'ham' ... 'spam' 'ham' 'ham']\n",
      "Actual values:\t ['ham' 'ham' 'ham' ... 'spam' 'ham' 'ham']\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham       1.00      1.00      1.00      3870\n",
      "        spam       0.98      0.97      0.98       587\n",
      "\n",
      "    accuracy                           0.99      4457\n",
      "   macro avg       0.99      0.98      0.99      4457\n",
      "weighted avg       0.99      0.99      0.99      4457\n",
      "\n",
      "Confusion Matrix:\n",
      " [[3860   10]\n",
      " [  18  569]]\n",
      "\n",
      "Accuracy: 99.37177473636976 %\n"
     ]
    }
   ],
   "source": [
    "# Naive Bayes Model assessment for train data set\n",
    "print('Naive Bayes Model Assessment\\nTrain data set:\\n')\n",
    "\n",
    "# Predictions\n",
    "print('Predictions:\\t', classifierNB.predict(x_train))\n",
    "\n",
    "# Actual values\n",
    "print('Actual values:\\t', y_train.values)\n",
    "print()\n",
    "\n",
    "# Model assessment results\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "prediction = classifierNB.predict(x_train)\n",
    "print(classification_report(y_train, prediction))\n",
    "print('Confusion Matrix:\\n', confusion_matrix(y_train, prediction))\n",
    "print('\\nAccuracy:', accuracy_score(y_train, prediction)*100, '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NIIFftxODiRB",
    "outputId": "da2e55a9-94ba-44ae-f632-aacc7ecc7500"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes Model Assessment\n",
      "Test data set:\n",
      "\n",
      "Predictions:\t ['ham' 'spam' 'ham' ... 'ham' 'ham' 'ham']\n",
      "Actual values:\t ['ham' 'spam' 'ham' ... 'ham' 'spam' 'ham']\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham       0.99      0.98      0.99       955\n",
      "        spam       0.91      0.96      0.93       160\n",
      "\n",
      "    accuracy                           0.98      1115\n",
      "   macro avg       0.95      0.97      0.96      1115\n",
      "weighted avg       0.98      0.98      0.98      1115\n",
      "\n",
      "Confusion Matrix:\n",
      " [[939  16]\n",
      " [  7 153]]\n",
      "\n",
      "Accuracy: 97.9372197309417 %\n"
     ]
    }
   ],
   "source": [
    "# Model assessment for test data set\n",
    "print('Naive Bayes Model Assessment\\nTest data set:\\n')\n",
    "\n",
    "# Predictions\n",
    "print('Predictions:\\t', classifierNB.predict(x_test))\n",
    "\n",
    "# Actual values\n",
    "print('Actual values:\\t', y_test.values)\n",
    "print()\n",
    "\n",
    "# Model assessment results\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "prediction = classifierNB.predict(x_test)\n",
    "print(classification_report(y_test, prediction))\n",
    "print('Confusion Matrix:\\n', confusion_matrix(y_test, prediction))\n",
    "print('\\nAccuracy:', accuracy_score(y_test, prediction)*100, '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TtFv2q8ohU5F"
   },
   "source": [
    "The support vector machine (SVM) analyses the data, determines the decision boundaries, and uses kernels to compute in input space. SVM is used for classification and regression, which are useful in statistical learning theory, and it assists in accurately detecting the components that must be taken into account in order to properly comprehend it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7-W02t0WEWeM",
    "outputId": "c1498ef4-b090-4ecd-840d-73812a17b5b8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(random_state=0)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Support Vector Machine (SVM) Classifier\n",
    "from sklearn import svm\n",
    "from sklearn.svm import SVC\n",
    "classifierSVM = SVC(kernel='rbf',random_state=0)\n",
    "classifierSVM.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4AW9jsL-FLB7",
    "outputId": "1ac9989d-dfaa-4d77-cd84-2c4454d1c9da"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Model Assessment\n",
      "Train data set:\n",
      "\n",
      "Predictions:\t ['ham' 'ham' 'ham' ... 'spam' 'ham' 'ham']\n",
      "Actual values:\t ['ham' 'ham' 'ham' ... 'spam' 'ham' 'ham']\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham       0.99      1.00      1.00      3870\n",
      "        spam       1.00      0.96      0.98       587\n",
      "\n",
      "    accuracy                           0.99      4457\n",
      "   macro avg       1.00      0.98      0.99      4457\n",
      "weighted avg       0.99      0.99      0.99      4457\n",
      "\n",
      "Confusion Matrix:\n",
      " [[3870    0]\n",
      " [  25  562]]\n",
      "\n",
      "Accuracy: 99.43908458604443 %\n"
     ]
    }
   ],
   "source": [
    "# SVM Model assessment for train data set\n",
    "print('SVM Model Assessment\\nTrain data set:\\n')\n",
    "\n",
    "# Predictions\n",
    "print('Predictions:\\t', classifierSVM.predict(x_train))\n",
    "\n",
    "# Actual values\n",
    "print('Actual values:\\t', y_train.values)\n",
    "print()\n",
    "\n",
    "# Model assessment results\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "prediction = classifierSVM.predict(x_train)\n",
    "print(classification_report(y_train, prediction))\n",
    "print('Confusion Matrix:\\n', confusion_matrix(y_train, prediction))\n",
    "print('\\nAccuracy:', accuracy_score(y_train, prediction)*100, '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oDTn5jtGFpay",
    "outputId": "0bfee2c0-53e9-4570-8cf7-ab8a28055b7e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Model Assessment\n",
      "Test data set:\n",
      "\n",
      "Predictions:\t ['ham' 'spam' 'ham' ... 'ham' 'ham' 'ham']\n",
      "Actual values:\t ['ham' 'spam' 'ham' ... 'ham' 'spam' 'ham']\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham       0.97      1.00      0.99       955\n",
      "        spam       1.00      0.84      0.91       160\n",
      "\n",
      "    accuracy                           0.98      1115\n",
      "   macro avg       0.99      0.92      0.95      1115\n",
      "weighted avg       0.98      0.98      0.98      1115\n",
      "\n",
      "Confusion Matrix:\n",
      " [[955   0]\n",
      " [ 26 134]]\n",
      "\n",
      "Accuracy: 97.66816143497758 %\n"
     ]
    }
   ],
   "source": [
    "# Model assessment for test data set\n",
    "print('SVM Model Assessment\\nTest data set:\\n')\n",
    "\n",
    "# Predictions\n",
    "print('Predictions:\\t', classifierSVM.predict(x_test))\n",
    "\n",
    "# Actual values\n",
    "print('Actual values:\\t', y_test.values)\n",
    "print()\n",
    "\n",
    "# Model assessment results\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "prediction = classifierSVM.predict(x_test)\n",
    "print(classification_report(y_test, prediction))\n",
    "print('Confusion Matrix:\\n', confusion_matrix(y_test, prediction))\n",
    "print('\\nAccuracy:', accuracy_score(y_test, prediction)*100, '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pp8KXeUlha80"
   },
   "source": [
    "The Naïve Bayes Classifier achieved an accuracy score of 99.37% on the train data and 97.34% on the test data. Meanwhile, the SVM Classifier achieved an accuracy score of 99.44% on the train data and 97.67% on the test data. It can be observed that the SVM classifier outperformed the Naïve Bayes Classifier in terms of the accuracy score, although the two models have an insignificant difference in their accuracy scores. Therefore, as a conclusion, the experiment results demonstrates that the SVM models have a higher spam-detection accuracy than the Naïve Bayes model."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "asgn1_EmailSpam.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
